---
title: "DMML Group 31 project"
output: pdf_document
date: "`r Sys.Date()`"

---

#Introduction

This analysis focuses on understanding the measurements that can help us predict the drug use by using several machine learning classification techniques. The dataset assigned to our group contains 1885 records who have different categories of drug use. We aims to use at least four machine learning techniques to predict the drug use based on the measurements given in dataset.

#Building models

We first use bagging forest and randomforest models.

```{r}
#| label: libraries
library(readr)
library(dplyr)
library(pROC)
library(randomForest)
library(caret)
```


```{r}
# Merging the seven categories of drug use into three 

drug_data<-read.csv("C:/Users/丁乐之2024/OneDrive/Documents/group_31.csv")
drug_data <- drug_data %>%
     mutate(Drug_Use_Category = case_when(
         Choc == "CL0" ~ 1,  # never used
         Choc %in% c("CL1", "CL2") ~ 2,  #  used over a decade over
         Choc %in% c("CL3", "CL4", "CL5", "CL6") ~ 3  # used in last decade
     ))%>%
    select(-Choc,-ID)

#Converting binary variables into factors

drug_data$Gender <- as.factor(drug_data$Gender)
drug_data$Education <- as.factor(drug_data$Education)
drug_data$Country <- as.factor(drug_data$Country)
drug_data$Ethnicity <- as.factor(drug_data$Ethnicity)
drug_data$Drug_Use_Category <- as.factor(drug_data$Drug_Use_Category)



#Splitting drug_data dataset into training,testing and validating sets, to improving models performance in prediction, we use stratification sampling

set.seed(42)
ind1 <- createDataPartition(drug_data$Drug_Use_Category, p = 0.5, list = FALSE)
ind2 <- createDataPartition(drug_data$Drug_Use_Category[-ind1], p = 0.5, list = FALSE)
ind3<- setdiff(1:nrow(drug_data), c(ind1, ind2))


drug_data_train<- drug_data[ind1, ]
drug_data_val <- drug_data[ind2, ]
drug_data_test <- drug_data[ind3, ]


# building bagging and randomforest tree


set.seed(42)
bagging <- randomForest(Drug_Use_Category~., data=drug_data_train,importance = TRUE,mtry=4, ntree=200)
rf <- randomForest(Drug_Use_Category~., data=drug_data_train,importance = TRUE,  ntree=200)





# validating model

bagging_testpro <- predict(bagging, drug_data_test, type = "prob")
rf_testpro <- predict(rf, drug_data_test, type = "prob")

bagging_test_auc <- multiclass.roc(drug_data_test$Drug_Use_Category, bagging_testpro[, 1])
rf_test_auc <- multiclass.roc(drug_data_test$Drug_Use_Category, rf_testpro[, 1])

print(paste("Bagging Test AUC:", bagging_test_auc$auc))
print(paste("Random Forest Test AUC:", rf_test_auc$auc))
```

According to the prediction correct rate, the Bagging tree model preforms slightly better than randomforest model. So I choose Bagging tree model to predict drug_data_categories.

Here we tried to filter the variables by mean GINI Importance, we select the variables that has mean Gini
importance scores >2, and see if it can improve the models performances.

```{r}

# Filtering factors by GINI mean score
importance(bagging)
important_vars_bagging <- names(which(importance(bagging)[, "MeanDecreaseGini"] > 3))
print(important_vars_bagging)

# Building the models by using the factors after modeling
bagging <- randomForest(Drug_Use_Category~Age+Education+Nscore+Escore+Oscore+Ascore+Cscore+Impulsive+SS, data=drug_data_train,importance = TRUE,mtry=4, ntree=200)
rf <- randomForest(Drug_Use_Category~Age+Education+Nscore+Escore+Oscore+Ascore+Cscore+Impulsive+SS, data=drug_data_train,importance = TRUE,ntree=200)

bagging_testpro <- predict(bagging, drug_data_test, type = "prob")
rf_testpro <- predict(rf, drug_data_test, type = "prob")

bagging_test_auc <- multiclass.roc(drug_data_test$Drug_Use_Category, bagging_testpro[, 1])
rf_test_auc <- multiclass.roc(drug_data_test$Drug_Use_Category, rf_testpro[, 1])

print(paste("Bagging Test AUC:", bagging_test_auc$auc))
print(paste("Random Forest Test AUC:", rf_test_auc$auc))


```

The AUC scores decrease compare to the models which contain all variables. So we choose the models containing all variables.

Here we used classification tree.

```{r setup, include=FALSE}
#|label:Load necessary libraries
library(readr)
library(dplyr)
library(GGally)
library(rpart)
library(rpart.plot)
library(caret)
knitr::opts_chunk$set(fig.path = "classification_tree_files/figure-pdf/", dev = "pdf")
```

```{r}
#|label:Load data and data preprocessing
# Load data
data<-read.csv("C:/Users/丁乐之2024/OneDrive/Desktop/DMML Group 31 Project/group_31.csv")
str(data)
summary(data)
# Data cleaning
clean_data <- na.omit(data)
table(clean_data$Choc)
# merging the seven categories of drug use into three 
merged_data <- clean_data %>%
    mutate(Choc = case_when(
        Choc == "CL0" ~ "Never Used",
        Choc %in% c("CL1", "CL2") ~ "Used Over a Decade Ago",
        Choc %in% c("CL3", "CL4", "CL5", "CL6") ~ "Used in Last Decade"
    )) %>%
    mutate(Choc = factor(Choc, levels = c("Never Used", "Used Over a Decade Ago", "Used in Last Decade")))
table(merged_data$Choc)
# Convert categorical variables to factors
merged_data$Gender <- as.factor(merged_data$Gender)      
merged_data$Education <- as.factor(merged_data$Education)  
merged_data$Country <- as.factor(merged_data$Country)    
merged_data$Ethnicity <- as.factor(merged_data$Ethnicity)  
merged_data$Choc <- as.factor(merged_data$Choc)
```
```{r}
#|label:Split the data into training and testing sets with stratified sampling for class balance
set.seed(1)
trainIndex <- createDataPartition(merged_data$Choc, p = 0.8, list = FALSE)
train_data <- merged_data[trainIndex, ]
test_data  <- merged_data[-trainIndex, ] 
```


```{r}
#|label:Build a classification tree model  
# Assign higher weights to "Used Over a Decade Ago" class samples to address class imbalance
weights <- ifelse(train_data$Choc == "Used Over a Decade Ago", 2, 1)
# Train a classification tree model with weighted data, controlling tree complexity and handling class imbalance
Model <- rpart(Choc ~ Age + Gender + Education + Country + Ethnicity + 
               Nscore + Escore + Oscore + Ascore + Cscore + Impulsive + SS, 
               data = train_data, method = "class", 
               control = rpart.control(cp = 0.001), 
               weights = weights)
# Print the model summary
summary(Model)
# Visualize the decision tree
rpart.plot(Model, type = 2, extra = 4, box.palette = "Blues")
# Make predictions on the test data
Ynew.pred <- predict(Model, newdata = test_data, type = "class")
# Evaluate the model performance (Confusion Matrix)
confusion_matrix <- table(Predicted = Ynew.pred, Actual = test_data$Choc)
print(confusion_matrix)
# Calculate accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy:", accuracy))
```
```{r}
data <- read.csv("C:/Users/丁乐之2024/OneDrive/Desktop/DMML Group 31 Project/group_31.csv")
skim(data)
ggpairs(data, columns=2:13, ggplot2::aes(colour=Choc, alpha=0.2))
warnings()

# Using the KNN method to cataegories of drug use 
set.seed(1)
n <- nrow(data)
ind1 <- sample(c(1:n), floor(0.5*n))
ind2 <- sample(c(1:n)[-ind1], floor(0.25*n))
ind3 <- setdiff(c(1:n),c(ind1,ind2))
data.train <- data[ind1,]
data.valid <- data[ind2,]
data.test  <- data[ind3,]

head(data)
var.mean <- apply(data.train[,2:13],2,mean)
var.sd   <- apply(data.train[,2:13],2,sd)

data.train.scale <-t(apply(data.train[,2:13], 1, function(x) (x-var.mean)/var.sd))
data.valid.scale <-t(apply(data.valid[,2:13], 1, function(x) (x-var.mean)/var.sd))
data.test.scale  <-t(apply(data.test[,2:13],  1, function(x) (x-var.mean)/var.sd))

library(class)
set.seed(1)
K <- c(1:15)
valid.corr <- c()
for (k in K){
  valid.pred <- knn(data.train.scale, data.valid.scale, data.train[,14], k=k)
  valid.corr[k] <- mean(data.valid[,14] == valid.pred)
}

plot(K, valid.corr, type="b", ylab="validation correct classification rate")


k.opt <- which.max(valid.corr)
test.pred <- knn(data.train.scale, data.test.scale, data.train[,14], k=k.opt)
table(data.test$Choc,test.pred)
```


